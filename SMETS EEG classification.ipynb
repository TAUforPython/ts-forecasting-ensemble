{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPzpFIEyGvqf4iro1vlfUhT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAUforPython/ts-forecasting-ensemble/blob/master/SMETS%20EEG%20classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# EEG Seizure Detection with SMETS - Google Colab Version\n",
        "# Dataset: CHB-MIT Scalp EEG Database (PhysioNet)\n",
        "# =============================================================================\n",
        "\n",
        "# 1. –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô\n",
        "!pip install -q mne pywt tslearn pyedflib joblib matplotlib scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzjdZTJzAw6w",
        "outputId": "0dc0e5ba-de71-4b71-946e-ccca83c99b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pywt (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pywt\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tslearn -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed-NGixFBaAd",
        "outputId": "5847693d-7da5-4da9-d5f4-257b12ba8842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/372.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.4/372.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m372.7/372.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mne -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC5Sjy0SDuM6",
        "outputId": "33a2b6ff-9368-4fb3-b5d2-d9dd8b8f233a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyWavelets -q"
      ],
      "metadata": {
        "id": "Z5VRwTgsDlWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 2. –ò–ú–ü–û–†–¢–´\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.signal as signal\n",
        "from scipy.stats import entropy\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "from time import time  # ‚Üê –í–ê–ñ–ù–û: –±—ã–ª–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "import logging\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from joblib import Parallel, delayed\n",
        "from tslearn.metrics import dtw\n",
        "import mne\n",
        "from mne.preprocessing import ICA\n",
        "import pywt"
      ],
      "metadata": {
        "id": "wqBcfFJ_BUsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. GOOGLE DRIVE & DATA SETUP\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# –ü—É—Ç–∏ –≤ Colab\n",
        "BASE_DIR = '/content/drive/MyDrive/edf_process/EEG_SMETS'  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–π –ø—É—Ç—å\n",
        "DATA_DIR = '/content/chbmit_data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(BASE_DIR, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5zpuThsEHjh",
        "outputId": "ef7df5b0-331b-43e6-c2bf-cd387a5ddfe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# code 1"
      ],
      "metadata": {
        "id": "41aLYH2cOzQj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgu3Bypu30kc",
        "outputId": "2bc00872-458b-4e86-c1e8-a45ffd17a039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...\n",
            "‚úì –°–∫–∞—á–∞–Ω summary: chb01-summary.txt\n",
            "‚úì –°–∫–∞—á–∞–Ω—ã EDF-—Ñ–∞–π–ª—ã –¥–ª—è chb01\n",
            "‚úì –°–∫–∞—á–∞–Ω summary: chb02-summary.txt\n",
            "‚úì –°–∫–∞—á–∞–Ω—ã EDF-—Ñ–∞–π–ª—ã –¥–ª—è chb02\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# 4. –§–£–ù–ö–¶–ò–ò –ó–ê–ì–†–£–ó–ö–ò –î–ê–ù–ù–´–• –° PHYSIONET\n",
        "def download_chbmit_files(patient_ids, output_dir):\n",
        "    \"\"\"\n",
        "    –ó–∞–≥—Ä—É–∑–∫–∞ EDF –∏ summary-—Ñ–∞–π–ª–æ–≤ —Å PhysioNet –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤\n",
        "    patient_ids: —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ ['01', '02', ...]\n",
        "    \"\"\"\n",
        "    base_url = \"https://physionet.org/files/chbmit/1.0.0\"\n",
        "\n",
        "    for pid in patient_ids:\n",
        "        patient_dir = os.path.join(output_dir, f\"chb{pid}\")\n",
        "        os.makedirs(patient_dir, exist_ok=True)\n",
        "\n",
        "        # –°–∫–∞—á–∏–≤–∞–µ–º summary-—Ñ–∞–π–ª\n",
        "        summary_url = f\"{base_url}/chb{pid}/chb{pid}-summary.txt\"\n",
        "        summary_path = os.path.join(patient_dir, f\"chb{pid}-summary.txt\")\n",
        "        if not os.path.exists(summary_path):\n",
        "            !wget -q -O {summary_path} {summary_url}\n",
        "            print(f\"‚úì –°–∫–∞—á–∞–Ω summary: chb{pid}-summary.txt\")\n",
        "\n",
        "        # –°–∫–∞—á–∏–≤–∞–µ–º –≤—Å–µ EDF-—Ñ–∞–π–ª—ã –ø–∞—Ü–∏–µ–Ω—Ç–∞\n",
        "        !wget -q -nH --cut-dirs=4 -P {patient_dir} -A \"chb{pid}_*.edf\" {base_url}/chb{pid}/\n",
        "        print(f\"‚úì –°–∫–∞—á–∞–Ω—ã EDF-—Ñ–∞–π–ª—ã –¥–ª—è chb{pid}\")\n",
        "\n",
        "def parse_chbmit_summary(summary_path, target_edf_filename):\n",
        "    \"\"\"\n",
        "    –ü–∞—Ä—Å–∏–Ω–≥ summary-—Ñ–∞–π–ª–∞ CHB-MIT –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –ø—Ä–∏—Å—Ç—É–ø–æ–≤\n",
        "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (start_sec, end_sec)\n",
        "    \"\"\"\n",
        "    intervals = []\n",
        "    try:\n",
        "        with open(summary_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ñ–∞–π–ª–∞–º\n",
        "        file_blocks = re.split(r'File Name:\\s*', content)\n",
        "\n",
        "        for block in file_blocks:\n",
        "            if target_edf_filename in block:\n",
        "                lines = block.strip().split('\\n')\n",
        "                i = 0\n",
        "                while i < len(lines):\n",
        "                    line = lines[i].strip()\n",
        "                    if 'Seizure' in line and 'Start Time' in line:\n",
        "                        # –§–æ—Ä–º–∞—Ç: \"Seizure 1 Start Time: 2345 seconds\"\n",
        "                        match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*seconds?', line)\n",
        "                        if match:\n",
        "                            start = float(match.group(1))\n",
        "                            # –ò—â–µ–º End Time –≤ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö\n",
        "                            for j in range(i+1, min(i+3, len(lines))):\n",
        "                                end_match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*seconds?', lines[j])\n",
        "                                if end_match and 'End Time' in lines[j]:\n",
        "                                    end = float(end_match.group(1))\n",
        "                                    intervals.append((start, end))\n",
        "                                    break\n",
        "                    i += 1\n",
        "                break\n",
        "    except Exception as e:\n",
        "        logger.error(f\"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {summary_path}: {e}\")\n",
        "\n",
        "    return intervals\n",
        "\n",
        "def get_all_edf_files(data_dir):\n",
        "    \"\"\"–ü–æ–∏—Å–∫ –≤—Å–µ—Ö EDF-—Ñ–∞–π–ª–æ–≤ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ CHB-MIT\"\"\"\n",
        "    edf_files = []\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for f in files:\n",
        "            if f.endswith('.edf') and not f.endswith('.seizures'):\n",
        "                edf_files.append(os.path.join(root, f))\n",
        "    return edf_files\n",
        "\n",
        "def classify_files(edf_files, data_dir):\n",
        "    \"\"\"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –Ω–∞ seizure/normal –Ω–∞ –æ—Å–Ω–æ–≤–µ summary-—Ñ–∞–π–ª–æ–≤\"\"\"\n",
        "    seizure_files = []\n",
        "    normal_files = []\n",
        "\n",
        "    for edf_path in edf_files:\n",
        "        filename = os.path.basename(edf_path)\n",
        "        patient_id = os.path.basename(os.path.dirname(edf_path))  # chbXX\n",
        "        summary_path = os.path.join(data_dir, patient_id, f\"{patient_id}-summary.txt\")\n",
        "\n",
        "        if os.path.exists(summary_path):\n",
        "            intervals = parse_chbmit_summary(summary_path, filename)\n",
        "            if intervals:\n",
        "                seizure_files.append(edf_path)\n",
        "            else:\n",
        "                normal_files.append(edf_path)\n",
        "        else:\n",
        "            # –ï—Å–ª–∏ summary –Ω–µ—Ç - —Å—á–∏—Ç–∞–µ–º normal (fallback)\n",
        "            normal_files.append(edf_path)\n",
        "\n",
        "    return normal_files, seizure_files\n",
        "\n",
        "# 5. –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(os.path.join(BASE_DIR, 'processing_log.txt')),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# =============================================================================\n",
        "# –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –ª–æ–≥–∏–∫–∏, –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø—É—Ç–∏)\n",
        "# =============================================================================\n",
        "\n",
        "def load_eeg(file_path):\n",
        "    try:\n",
        "        raw = mne.io.read_raw_edf(file_path, preload=True, verbose=False)\n",
        "        return raw\n",
        "    except Exception as e:\n",
        "        logger.error(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def filter_artifacts(raw):\n",
        "    if raw is None:\n",
        "        return None\n",
        "    raw.filter(l_freq=0.5, h_freq=70, method=\"iir\")\n",
        "    raw.notch_filter(freqs=50)\n",
        "\n",
        "    ica = ICA(n_components=20, random_state=42, max_iter='auto')\n",
        "    ica.fit(raw)\n",
        "    ica_sources = ica.get_sources(raw).get_data()\n",
        "    variances = np.var(ica_sources, axis=1)\n",
        "    exclude = np.argsort(variances)[-2:]\n",
        "    logger.info(f\"–ò—Å–∫–ª—é—á–µ–Ω—ã ICA-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: {exclude}\")\n",
        "    raw = ica.apply(raw, exclude=exclude)\n",
        "    return raw\n",
        "\n",
        "def normalize_data(raw):\n",
        "    if raw is None:\n",
        "        return None\n",
        "    data = raw.get_data()\n",
        "    mean = np.mean(data, axis=1, keepdims=True)\n",
        "    std = np.std(data, axis=1, keepdims=True)\n",
        "    normalized_data = (data - mean) / (std + 1e-10)\n",
        "    raw._data = normalized_data\n",
        "    return raw\n",
        "\n",
        "def segment_data(raw, window_size=1):\n",
        "    if raw is None:\n",
        "        return []\n",
        "    sfreq = raw.info[\"sfreq\"]\n",
        "    window_samples = int(window_size * sfreq)\n",
        "    data = raw.get_data()\n",
        "    n_samples = data.shape[1]\n",
        "    segments = []\n",
        "    for start in range(0, n_samples, window_samples):\n",
        "        end = start + window_samples\n",
        "        if end <= n_samples:\n",
        "            segment = data[:, start:end]\n",
        "            segments.append(segment)\n",
        "    logger.info(f\"–°–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}, sfreq: {sfreq}, window: {window_samples}\")\n",
        "    return segments\n",
        "\n",
        "def quality_control(segments):\n",
        "    if not segments:\n",
        "        return []\n",
        "    clean_segments = []\n",
        "    for i, seg in enumerate(segments):\n",
        "        max_abs = np.max(np.abs(seg))\n",
        "        std = np.std(seg)\n",
        "        threshold = 10 * std\n",
        "        if max_abs < threshold:\n",
        "            clean_segments.append(seg)\n",
        "        else:\n",
        "            logger.info(f\"–°–µ–≥–º–µ–Ω—Ç {i} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω: max={max_abs:.2f}, thr={threshold:.2f}\")\n",
        "    logger.info(f\"QC: {len(segments)} ‚Üí {len(clean_segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤\")\n",
        "    return clean_segments\n",
        "\n",
        "def dwt_transform(segment, level=5, top_k=47):\n",
        "    if segment.size == 0:\n",
        "        return np.array([])\n",
        "    if segment.ndim == 1:\n",
        "        segment = segment[np.newaxis, :]\n",
        "    coeffs = pywt.wavedec(segment, 'db6', level=level, axis=-1)\n",
        "    all_coeffs = np.concatenate([c[np.newaxis, :] if c.ndim == 1 else c for c in coeffs], axis=-1)\n",
        "    if all_coeffs.ndim == 1:\n",
        "        all_coeffs = all_coeffs[np.newaxis, :]\n",
        "    top_indices = np.argsort(np.abs(all_coeffs), axis=-1)[:, -top_k:]\n",
        "    top_coeffs = np.take_along_axis(all_coeffs, top_indices, axis=-1)\n",
        "    return top_coeffs\n",
        "\n",
        "def compute_distances(coeffs1, coeffs2):\n",
        "    if coeffs1.ndim == 1: coeffs1 = coeffs1[np.newaxis, :]\n",
        "    if coeffs2.ndim == 1: coeffs2 = coeffs2[np.newaxis, :]\n",
        "    if coeffs1.shape != coeffs2.shape:\n",
        "        raise ValueError(f\"Shape mismatch: {coeffs1.shape} vs {coeffs2.shape}\")\n",
        "    return np.abs(coeffs1 - coeffs2)\n",
        "\n",
        "def pair_matching(distances):\n",
        "    if distances.size == 0:\n",
        "        return []\n",
        "    d = []\n",
        "    dist_copy = distances.copy()\n",
        "    while dist_copy.size > 0:\n",
        "        min_dist = np.min(dist_copy)\n",
        "        idx = np.unravel_index(np.argmin(dist_copy), dist_copy.shape)\n",
        "        d.append(min_dist)\n",
        "        dist_copy = np.delete(dist_copy, idx[0], axis=0)\n",
        "        dist_copy = np.delete(dist_copy, idx[1], axis=1)\n",
        "    return d\n",
        "\n",
        "def p_norm(d, p):\n",
        "    if not d:\n",
        "        return 0\n",
        "    return np.sum(np.abs(d) ** p) ** (1 / p)\n",
        "\n",
        "def entropy_penalty(unmatched, coeffs1):\n",
        "    EP = 0\n",
        "    if unmatched and coeffs1.size > 0:\n",
        "        for j in unmatched:\n",
        "            hist = np.histogram(j, bins=10, density=True)[0]\n",
        "            H_j = entropy(hist + 1e-10)  # avoid log(0)\n",
        "            min_d_j = min([np.min(np.sqrt(np.sum((j - c) ** 2))) for c in coeffs1])\n",
        "            denom = sum([entropy(np.histogram(c, bins=10, density=True)[0] + 1e-10) for c in coeffs1])\n",
        "            RE_j = min_d_j * H_j / denom if denom != 0 else 0\n",
        "            EP += RE_j\n",
        "    return EP\n",
        "\n",
        "def dimensionality_penalty(m, n):\n",
        "    return (n - m) / (n + m) if (m + n) > 0 else 0\n",
        "\n",
        "def SMETS(segment1, segment2):\n",
        "    if segment1.size == 0 or segment2.size == 0:\n",
        "        return 0, 0, 0\n",
        "    coeffs1 = dwt_transform(segment1)\n",
        "    coeffs2 = dwt_transform(segment2)\n",
        "    if coeffs1.size == 0 or coeffs2.size == 0:\n",
        "        return 0, 0, 0\n",
        "\n",
        "    distances = compute_distances(coeffs1, coeffs2)\n",
        "    d = pair_matching(distances)\n",
        "    p = min(len(segment1), len(segment2))\n",
        "    d_norm = p_norm(d, p)\n",
        "\n",
        "    m, n = len(segment1), len(segment2)\n",
        "    EP = 0\n",
        "    if m < n:\n",
        "        unmatched = [coeffs2[i] for i in range(m, n)]\n",
        "        EP = entropy_penalty(unmatched, coeffs1)\n",
        "    elif n < m:\n",
        "        unmatched = [coeffs1[i] for i in range(n, m)]\n",
        "        EP = entropy_penalty(unmatched, coeffs2)\n",
        "\n",
        "    P = dimensionality_penalty(m, n)\n",
        "    total_dist = np.sqrt((d_norm + 0.01 * EP) ** 2 + (0.01 * P)**2)\n",
        "    return total_dist, P, EP\n",
        "\n",
        "def compute_smets_to_reference(i, segment, reference_segment):\n",
        "    total_dist, _, _ = SMETS(segment, reference_segment)\n",
        "    return (i, total_dist)\n",
        "\n",
        "def merge_intervals(intervals, max_gap=5):\n",
        "    if not intervals:\n",
        "        return []\n",
        "    merged = []\n",
        "    for start, end in sorted(intervals):\n",
        "        if not merged or merged[-1][1] + max_gap < start:\n",
        "            merged.append([start, end])\n",
        "        else:\n",
        "            merged[-1][1] = max(end, merged[-1][1])\n",
        "    return merged\n",
        "\n",
        "def filter_intervals(intervals, min_length=10):\n",
        "    return [i for i in intervals if i[1] - i[0] >= min_length]\n",
        "\n",
        "def extend_intervals(intervals, buffer=10, max_time=3600):\n",
        "    extended = []\n",
        "    for start, end in intervals:\n",
        "        start = max(0, start - buffer)\n",
        "        end = min(max_time, end + buffer)\n",
        "        extended.append((start, end))\n",
        "    return extended\n",
        "\n",
        "def evaluate_intervals(detected, true_intervals, total_duration):\n",
        "    if not true_intervals:\n",
        "        sens = 1.0\n",
        "        prec = 1.0 if not detected else 0.0\n",
        "        f1 = 2 * sens * prec / (sens + prec) if (sens + prec) > 0 else 0.0\n",
        "        return sens, prec, f1\n",
        "    if not detected:\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    tp = fp = fn = 0\n",
        "    for true_start, true_end in true_intervals:\n",
        "        found = False\n",
        "        for det_start, det_end in detected:\n",
        "            overlap_start = max(true_start, det_start)\n",
        "            overlap_end = min(true_end, det_end)\n",
        "            if overlap_start < overlap_end:\n",
        "                tp += overlap_end - overlap_start\n",
        "                found = True\n",
        "        if not found:\n",
        "            fn += true_end - true_start\n",
        "\n",
        "    for det_start, det_end in detected:\n",
        "        overlap_found = False\n",
        "        for true_start, true_end in true_intervals:\n",
        "            if max(true_start, det_start) < min(true_end, det_end):\n",
        "                overlap_found = True\n",
        "                break\n",
        "        if not overlap_found:\n",
        "            fp += det_end - det_start\n",
        "\n",
        "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    f1 = 2 * sens * prec / (sens + prec) if (sens + prec) > 0 else 0.0\n",
        "    logger.info(f\"TP={tp:.1f}, FP={fp:.1f}, FN={fn:.1f} | Sens={sens:.3f}, Prec={prec:.3f}, F1={f1:.3f}\")\n",
        "    return sens, prec, f1\n",
        "\n",
        "def detect_seizure_with_smets(segments, window_size, sfreq, max_duration, true_intervals, n_jobs=4):\n",
        "    start_time = time()\n",
        "\n",
        "    # –í—ã–±–æ—Ä —ç—Ç–∞–ª–æ–Ω–∞ (–Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã)\n",
        "    n_ref = min(100, len(segments))\n",
        "    ref_candidates = []\n",
        "    for i in range(n_ref):\n",
        "        seg_start = i * window_size\n",
        "        seg_end = (i + 1) * window_size\n",
        "        is_seizure = any(max(start, seg_start) < min(end, seg_end) for start, end in true_intervals)\n",
        "        if not is_seizure:\n",
        "            ref_candidates.append(segments[i])\n",
        "\n",
        "    if not ref_candidates:\n",
        "        ref_candidates = segments[:n_ref]\n",
        "        logger.warning(\"–ù–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è —ç—Ç–∞–ª–æ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ 100\")\n",
        "\n",
        "    amplitudes = [np.max(np.abs(seg)) for seg in ref_candidates]\n",
        "    q25 = np.percentile(amplitudes, 25)\n",
        "    ref_segments = [seg for seg, amp in zip(ref_candidates, amplitudes) if amp < q25]\n",
        "    reference_data = np.mean(ref_segments, axis=0) if ref_segments else segments[0]\n",
        "\n",
        "    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ SMETS\n",
        "    logger.info(f\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ SMETS –¥–ª—è {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤...\")\n",
        "    results = Parallel(n_jobs=n_jobs, verbose=0)(\n",
        "        delayed(compute_smets_to_reference)(i, segment, reference_data)\n",
        "        for i, segment in enumerate(segments)\n",
        "    )\n",
        "\n",
        "    smets_distances = sorted(results, key=lambda x: x[0])\n",
        "    smets_distances = [(i * window_size, dist) for i, dist in smets_distances]\n",
        "\n",
        "    # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ\n",
        "    distances = [d for _, d in smets_distances]\n",
        "    smoothed = uniform_filter1d(distances, size=3)\n",
        "    smets_distances = [(t, d) for (t, _), d in zip(smets_distances, smoothed)]\n",
        "\n",
        "    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ IQR\n",
        "    q75, q25 = np.percentile(smoothed, [75, 25])\n",
        "    iqr = q75 - q25\n",
        "    threshold = max(q75 + 2.5 * iqr, 250)\n",
        "    logger.info(f\"–ü–æ—Ä–æ–≥: {threshold:.2f} (Q75={q75:.2f}, IQR={iqr:.2f})\")\n",
        "\n",
        "    # –í—ã–¥–µ–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤\n",
        "    seizure_intervals = []\n",
        "    start = None\n",
        "    for (idx, _), dist in zip(smets_distances, smoothed):\n",
        "        if dist > threshold and start is None:\n",
        "            start = idx\n",
        "        elif dist <= threshold and start is not None:\n",
        "            if idx - start >= 10:\n",
        "                seizure_intervals.append((start, idx))\n",
        "            start = None\n",
        "    if start is not None and len(segments) * window_size - start >= 10:\n",
        "        seizure_intervals.append((start, len(segments) * window_size))\n",
        "\n",
        "    # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
        "    seizure_intervals = merge_intervals(seizure_intervals, max_gap=5)\n",
        "    seizure_intervals = filter_intervals(seizure_intervals, min_length=10)\n",
        "    seizure_intervals = extend_intervals(seizure_intervals, buffer=10, max_time=max_duration)\n",
        "\n",
        "    logger.info(f\"SMETS –≤—Ä–µ–º—è: {time() - start_time:.2f}—Å, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤: {len(seizure_intervals)}\")\n",
        "    return smets_distances, [threshold] * len(smets_distances), seizure_intervals\n",
        "\n",
        "def process_file(file_path, base_path, window_size, n_jobs):\n",
        "    start_time = time()\n",
        "    file_name = os.path.basename(file_path)\n",
        "    logger.info(f\"‚Üí –û–±—Ä–∞–±–æ—Ç–∫–∞: {file_name}\")\n",
        "\n",
        "    raw = load_eeg(file_path)\n",
        "    if raw is None:\n",
        "        return 0.0, 0.0, 0.0, time() - start_time, file_name\n",
        "\n",
        "    max_duration = raw.n_times / raw.info['sfreq']\n",
        "    raw = filter_artifacts(raw)\n",
        "    raw = normalize_data(raw)\n",
        "    segments = segment_data(raw, window_size)\n",
        "    segments = quality_control(segments)\n",
        "\n",
        "    # –ü–æ–∏—Å–∫ true intervals\n",
        "    patient_id = os.path.basename(os.path.dirname(file_path))\n",
        "    summary_path = os.path.join(base_path, patient_id, f\"{patient_id}-summary.txt\")\n",
        "    true_intervals = parse_chbmit_summary(summary_path, file_name) if os.path.exists(summary_path) else []\n",
        "\n",
        "    # –î–µ—Ç–µ–∫—Ü–∏—è\n",
        "    smets_dist, threshold, detected = detect_seizure_with_smets(\n",
        "        segments, window_size, raw.info['sfreq'], max_duration, true_intervals, n_jobs\n",
        "    )\n",
        "\n",
        "    # –û—Ü–µ–Ω–∫–∞\n",
        "    sens, prec, f1 = evaluate_intervals(detected, true_intervals, max_duration)\n",
        "\n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Google Drive)\n",
        "    times = [t for t, _ in smets_dist]\n",
        "    dists = [d for _, d in smets_dist]\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    plt.plot(times, dists, label='SMETS distance', linewidth=1)\n",
        "    plt.plot(times, threshold, 'r--', label='Adaptive threshold', alpha=0.7)\n",
        "\n",
        "    for s, e in detected:\n",
        "        plt.axvspan(s, e, color='orange', alpha=0.3, label='Detected' if s == (detected[0][0] if detected else 0) else \"\")\n",
        "    for s, e in true_intervals:\n",
        "        plt.axvspan(s, e, color='green', alpha=0.2, label='Ground truth' if s == (true_intervals[0][0] if true_intervals else 0) else \"\")\n",
        "\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('SMETS distance')\n",
        "    plt.title(f'Seizure Detection: {file_name}')\n",
        "    plt.legend(fontsize=8)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plot_path = os.path.join(BASE_DIR, f'plot_{file_name}.png')\n",
        "    plt.savefig(plot_path, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # –õ–æ–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "    with open(os.path.join(BASE_DIR, 'results_summary.txt'), 'a') as f:\n",
        "        f.write(f\"{file_name}\\t{sens:.3f}\\t{prec:.3f}\\t{f1:.3f}\\t{time()-start_time:.1f}s\\n\")\n",
        "\n",
        "    gc.collect()\n",
        "    return sens, prec, f1, time() - start_time, file_name\n",
        "\n",
        "def process_chbmit_data(base_path, window_size=1, n_jobs=4, max_patients=3, max_files_per_patient=2):\n",
        "    \"\"\"\n",
        "    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ CHB-MIT dataset –≤ Colab\n",
        "    \"\"\"\n",
        "    total_start = time()\n",
        "    logger.info(f\"üöÄ –°—Ç–∞—Ä—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ | patients‚â§{max_patients}, files/patient‚â§{max_files_per_patient}\")\n",
        "\n",
        "    # –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–ø—Ä–∏–º–µ—Ä: –ø–∞—Ü–∏–µ–Ω—Ç—ã 01-03)\n",
        "    patient_ids = [f\"{i:02d}\" for i in range(1, max_patients+1)]\n",
        "    download_chbmit_files(patient_ids, DATA_DIR)\n",
        "\n",
        "    # –ü–æ–∏—Å–∫ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤\n",
        "    all_edf = get_all_edf_files(DATA_DIR)\n",
        "    normal_files, seizure_files = classify_files(all_edf, DATA_DIR)\n",
        "\n",
        "    logger.info(f\"üìÅ –ù–∞–π–¥–µ–Ω–æ: {len(normal_files)} normal, {len(seizure_files)} seizure —Ñ–∞–π–ª–æ–≤\")\n",
        "\n",
        "    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞\n",
        "    seizure_files = seizure_files[:max_files_per_patient * len(patient_ids)]\n",
        "    normal_files = normal_files[:max_files_per_patient * len(patient_ids)]\n",
        "\n",
        "    files_to_process = seizure_files + normal_files\n",
        "    logger.info(f\"‚öôÔ∏è  –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(files_to_process)} —Ñ–∞–π–ª–æ–≤\")\n",
        "\n",
        "    # –û–±—Ä–∞–±–æ—Ç–∫–∞\n",
        "    results = []\n",
        "    for i, fpath in enumerate(files_to_process, 1):\n",
        "        logger.info(f\"[{i}/{len(files_to_process)}] {os.path.basename(fpath)}\")\n",
        "        sens, prec, f1, t, name = process_file(fpath, DATA_DIR, window_size, n_jobs)\n",
        "        results.append((name, sens, prec, f1, t))\n",
        "        gc.collect()\n",
        "\n",
        "    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç\n",
        "    report_path = os.path.join(BASE_DIR, 'final_report.txt')\n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(\"CHB-MIT Seizure Detection Results (SMETS)\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\")\n",
        "        f.write(f\"{'File':<30} {'Sens':>8} {'Prec':>8} {'F1':>8} {'Time':>10}\\n\")\n",
        "        f.write(\"-\"*60 + \"\\n\")\n",
        "        for name, s, p, f1, t in results:\n",
        "            f.write(f\"{name:<30} {s:>8.3f} {p:>8.3f} {f1:>8.3f} {t:>9.1f}s\\n\")\n",
        "        f.write(\"-\"*60 + \"\\n\")\n",
        "\n",
        "        # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "        if results:\n",
        "            avg_sens = np.mean([r[1] for r in results])\n",
        "            avg_prec = np.mean([r[2] for r in results])\n",
        "            avg_f1 = np.mean([r[3] for r in results])\n",
        "            f.write(f\"\\n{'AVERAGE':<30} {avg_sens:>8.3f} {avg_prec:>8.3f} {avg_f1:>8.3f}\\n\")\n",
        "            f.write(f\"Total time: {time() - total_start:.1f} seconds\\n\")\n",
        "\n",
        "    logger.info(f\"‚úÖ –ì–æ—Ç–æ–≤–æ! –û—Ç—á—ë—Ç: {report_path}\")\n",
        "    logger.info(f\"üìä –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {BASE_DIR}\")\n",
        "\n",
        "    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
        "    # !rm -rf /content/chbmit_data\n",
        "\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# –ó–ê–ü–£–°–ö\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏\n",
        "    WINDOW_SIZE = 1  # —Å–µ–∫—É–Ω–¥—ã\n",
        "    N_JOBS = 2       # Colab: –Ω–µ —Å—Ç–∞–≤—å—Ç–µ >2-4 –∏–∑-–∑–∞ –ª–∏–º–∏—Ç–æ–≤ CPU\n",
        "    MAX_PATIENTS = 2 # –Ω–∞—á–Ω–∏—Ç–µ —Å –º–∞–ª–æ–≥–æ –¥–ª—è —Ç–µ—Å—Ç–∞\n",
        "    MAX_FILES = 1    # —Ñ–∞–π–ª–æ–≤ –Ω–∞ –ø–∞—Ü–∏–µ–Ω—Ç–∞\n",
        "\n",
        "    print(\"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...\")\n",
        "    results = process_chbmit_data(\n",
        "        base_path=DATA_DIR,\n",
        "        window_size=WINDOW_SIZE,\n",
        "        n_jobs=N_JOBS,\n",
        "        max_patients=MAX_PATIENTS,\n",
        "        max_files_per_patient=MAX_FILES\n",
        "    )\n",
        "\n",
        "    # –ë—ã—Å—Ç—Ä—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "    if results:\n",
        "        print(\"\\nüìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\")\n",
        "        for name, s, p, f1, t in results:\n",
        "            print(f\"{name:25s} | F1: {f1:.3f} | Time: {t:.1f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# code 2"
      ],
      "metadata": {
        "id": "rJgmej2HO2ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.signal as signal\n",
        "from scipy.stats import entropy\n",
        "from scipy.ndimage import uniform_filter1d\n",
        "import pywt\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "import logging\n",
        "from joblib import Parallel, delayed\n",
        "from tslearn.metrics import dtw\n",
        "import pyedflib\n",
        "from time import time\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set working directory on Google Drive\n",
        "os.chdir('/content/drive/MyDrive/edf_process/EEG_SMETS')\n",
        "\n",
        "## Configuration and Logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - INFO - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Log to file on Google Drive\n",
        "file_handler = logging.FileHandler('/content/drive/MyDrive/edf_process/EEG_SMETS/generalized_processing_log.txt')\n",
        "file_handler.setLevel(logging.INFO)\n",
        "file_handler.setFormatter(logging.Formatter('%(asctime)s - INFO - %(message)s'))\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "## Data Loading Functions (from example)\n",
        "\n",
        "def read_edf(file_path):\n",
        "    \"\"\"Read EDF file using pyedflib\"\"\"\n",
        "    with pyedflib.EdfReader(file_path) as f:\n",
        "        n = f.signals_in_file\n",
        "        signal_labels = f.getSignalLabels()\n",
        "        signals = np.zeros((n, f.getNSamples()[0]))\n",
        "        for i in range(n):\n",
        "            signals[i, :] = f.readSignal(i)\n",
        "    return signals, signal_labels\n",
        "\n",
        "def parse_summary_file(summary_file_path):\n",
        "    \"\"\"Parse PhysioNet summary file to extract seizure information\"\"\"\n",
        "    edf_files = []\n",
        "    with open(summary_file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "        files = re.split(r'File Name: ', content)[1:]  # Split by files\n",
        "        for file_info in files:\n",
        "            lines = file_info.split('\\n')\n",
        "            file_name = lines[0].strip()\n",
        "            seizure_start = []\n",
        "            seizure_end = []\n",
        "            num_seizures = 0\n",
        "            k = 0\n",
        "            for line in lines[1:]:\n",
        "                if 'Number of Seizures in File' in line:\n",
        "                    num_seizures = int(line.split(':')[-1].strip())\n",
        "                    k = num_seizures\n",
        "                    continue\n",
        "\n",
        "                if k > 0:\n",
        "                    if ('Seizure' in line) and ('Start Time' in line):\n",
        "                        seizure_start_i = int(re.search(r'\\d+', line.split(':')[-1].strip()).group())\n",
        "                        seizure_start.append(seizure_start_i)\n",
        "                        continue\n",
        "\n",
        "                    if ('Seizure' in line) and ('End Time' in line):\n",
        "                        seizure_end_i = int(re.search(r'\\d+', line.split(':')[-1].strip()).group())\n",
        "                        seizure_end.append(seizure_end_i)\n",
        "                        k -= 1\n",
        "                        continue\n",
        "\n",
        "            edf_files.append({\n",
        "                'file_name': file_name,\n",
        "                'seizure_start': seizure_start,\n",
        "                'seizure_end': seizure_end,\n",
        "                'num_seizures': num_seizures\n",
        "            })\n",
        "    return edf_files\n",
        "\n",
        "## Signal Processing Functions\n",
        "\n",
        "def butter_bandpass_filter(data, fs, order=5):\n",
        "    \"\"\"Butterworth bandpass filter for EEG frequency bands\"\"\"\n",
        "    # Frequency bands: Delta, Theta, Alpha, Beta, Gamma\n",
        "    lowcut = [1, 4, 8, 14, 30]\n",
        "    highcut = [4, 8, 14, 30, 50]\n",
        "    nyquist = 0.5 * fs\n",
        "    filtered_signals = []\n",
        "\n",
        "    for i in range(len(lowcut)):\n",
        "        low = lowcut[i] / nyquist\n",
        "        high = highcut[i] / nyquist\n",
        "        b, a = signal.butter(order, [low, high], btype='band')\n",
        "        filtered_data = signal.lfilter(b, a, data)\n",
        "        filtered_signals.append(filtered_data)\n",
        "\n",
        "    return filtered_signals\n",
        "\n",
        "def measured(signal, window_size, sampling_time):\n",
        "    \"\"\"Remove DC offset using moving window\"\"\"\n",
        "    window_size = int(window_size * sampling_time)\n",
        "    n = signal.size // window_size\n",
        "    f_signal = np.copy(signal)\n",
        "    for i in range(int(n)):\n",
        "        a = i * window_size\n",
        "        b = (i + 1) * window_size\n",
        "        f_signal[a:b] = signal[a:b] - np.mean(signal[a:b])\n",
        "    return f_signal\n",
        "\n",
        "def mean_delete(filtered_signals, window_count, sampling_rate):\n",
        "    \"\"\"Remove mean from filtered signals\"\"\"\n",
        "    processed_signals = []\n",
        "    for filtered in filtered_signals:\n",
        "        filtered_mean = measured(filtered, window_count, sampling_rate)\n",
        "        processed_signals.append(filtered_mean)\n",
        "    return processed_signals\n",
        "\n",
        "## EEG Processing Functions (adapted for numpy arrays)\n",
        "\n",
        "def load_eeg_from_array(signals, sampling_rate=256):\n",
        "    \"\"\"Convert numpy array to mne-like structure\"\"\"\n",
        "    info = {\n",
        "        'sfreq': sampling_rate,\n",
        "        'n_times': signals.shape[1],\n",
        "        'ch_names': [f'CH{i}' for i in range(signals.shape[0])]\n",
        "    }\n",
        "    return signals, info\n",
        "\n",
        "def filter_artifacts(signals, sampling_rate=256):\n",
        "    \"\"\"Filter artifacts from EEG signals\"\"\"\n",
        "    if signals is None:\n",
        "        return None\n",
        "\n",
        "    # Apply bandpass filter (0.5-70 Hz)\n",
        "    nyquist = 0.5 * sampling_rate\n",
        "    low = 0.5 / nyquist\n",
        "    high = 70.0 / nyquist\n",
        "    b, a = signal.butter(5, [low, high], btype='band')\n",
        "    filtered = signal.lfilter(b, a, signals, axis=1)\n",
        "\n",
        "    # Notch filter at 50 Hz\n",
        "    b_notch, a_notch = signal.iirnotch(50.0, 30.0, sampling_rate)\n",
        "    filtered = signal.lfilter(b_notch, a_notch, filtered, axis=1)\n",
        "\n",
        "    return filtered\n",
        "\n",
        "def normalize_data(signals):\n",
        "    \"\"\"Z-score normalization\"\"\"\n",
        "    if signals is None:\n",
        "        return None\n",
        "    mean = np.mean(signals, axis=1, keepdims=True)\n",
        "    std = np.std(signals, axis=1, keepdims=True)\n",
        "    normalized = (signals - mean) / (std + 1e-10)\n",
        "    return normalized\n",
        "\n",
        "def segment_data(signals, sampling_rate, window_size=1):\n",
        "    \"\"\"Segment data into windows\"\"\"\n",
        "    if signals is None:\n",
        "        return []\n",
        "\n",
        "    window_samples = int(window_size * sampling_rate)\n",
        "    n_samples = signals.shape[1]\n",
        "    segments = []\n",
        "\n",
        "    for start in range(0, n_samples, window_samples):\n",
        "        end = start + window_samples\n",
        "        if end <= n_samples:\n",
        "            segment = signals[:, start:end]\n",
        "            segments.append(segment)\n",
        "\n",
        "    logger.info(f\"Created segments: {len(segments)}, sampling rate: {sampling_rate}, window: {window_samples} samples\")\n",
        "    return segments\n",
        "\n",
        "def quality_control(segments):\n",
        "    \"\"\"Filter out noisy segments\"\"\"\n",
        "    if not segments:\n",
        "        return []\n",
        "\n",
        "    clean_segments = []\n",
        "    for i, seg in enumerate(segments):\n",
        "        max_abs = np.max(np.abs(seg))\n",
        "        std = np.std(seg)\n",
        "        threshold = 10 * std\n",
        "        if max_abs < threshold:\n",
        "            clean_segments.append(seg)\n",
        "        else:\n",
        "            logger.info(f\"Segment {i} filtered: max_abs={max_abs:.2f}, threshold={threshold:.2f}\")\n",
        "\n",
        "    logger.info(f\"Total segments: {len(segments)}, Clean segments: {len(clean_segments)}\")\n",
        "    return clean_segments\n",
        "\n",
        "## SMETS Functions (unchanged logic)\n",
        "\n",
        "def dwt_transform(segment, level=5, top_k=47):\n",
        "    \"\"\"Discrete Wavelet Transform\"\"\"\n",
        "    if segment.size == 0:\n",
        "        return np.array([])\n",
        "    if segment.ndim == 1:\n",
        "        segment = segment[np.newaxis, :]\n",
        "\n",
        "    coeffs = pywt.wavedec(segment, 'db6', level=level, axis=-1)\n",
        "    all_coeffs = np.concatenate([c[np.newaxis, :] if c.ndim == 1 else c for c in coeffs], axis=-1)\n",
        "\n",
        "    if all_coeffs.ndim == 1:\n",
        "        all_coeffs = all_coeffs[np.newaxis, :]\n",
        "\n",
        "    top_indices = np.argsort(np.abs(all_coeffs), axis=-1)[:, -top_k:]\n",
        "    top_coeffs = np.take_along_axis(all_coeffs, top_indices, axis=-1)\n",
        "    return top_coeffs\n",
        "\n",
        "def compute_distances(coeffs1, coeffs2):\n",
        "    \"\"\"Compute distances between coefficients\"\"\"\n",
        "    if coeffs1.ndim == 1:\n",
        "        coeffs1 = coeffs1[np.newaxis, :]\n",
        "    if coeffs2.ndim == 1:\n",
        "        coeffs2 = coeffs2[np.newaxis, :]\n",
        "    if coeffs1.shape != coeffs2.shape:\n",
        "        raise ValueError(f\"Shape mismatch: coeffs1={coeffs1.shape}, coeffs2={coeffs2.shape}\")\n",
        "    return np.abs(coeffs1 - coeffs2)\n",
        "\n",
        "def pair_matching(distances):\n",
        "    \"\"\"Pair matching algorithm\"\"\"\n",
        "    if distances.size == 0:\n",
        "        return []\n",
        "\n",
        "    d = []\n",
        "    distances = distances.copy()\n",
        "    while distances.size > 0:\n",
        "        min_dist = np.min(distances)\n",
        "        idx = np.unravel_index(np.argmin(distances), distances.shape)\n",
        "        d.append(min_dist)\n",
        "        distances = np.delete(distances, idx[0], axis=0)\n",
        "        distances = np.delete(distances, idx[1], axis=1)\n",
        "    return d\n",
        "\n",
        "def p_norm(d, p):\n",
        "    \"\"\"p-norm calculation\"\"\"\n",
        "    if not d:\n",
        "        return 0\n",
        "    norm = np.sum(np.abs(d) ** p) ** (1 / p)\n",
        "    return norm\n",
        "\n",
        "def entropy_penalty(unmatched, coeffs1):\n",
        "    \"\"\"Entropy penalty for unmatched components\"\"\"\n",
        "    EP = 0\n",
        "    if unmatched and coeffs1.size > 0:\n",
        "        for j in unmatched:\n",
        "            H_j = entropy(np.histogram(j, bins=10, density=True)[0])\n",
        "            min_d_j = min([np.min(np.sqrt(np.sum((j - c) ** 2))) for c in coeffs1])\n",
        "            denom = sum([entropy(np.histogram(c, bins=10, density=True)[0]) for c in coeffs1])\n",
        "            RE_j = min_d_j * H_j / denom if denom != 0 else 0\n",
        "            EP += RE_j\n",
        "    return EP\n",
        "\n",
        "def dimensionality_penalty(m, n):\n",
        "    \"\"\"Penalty for dimensionality difference\"\"\"\n",
        "    return (n - m) / (n + m) if (m + n) > 0 else 0\n",
        "\n",
        "def SMETS(segment1, segment2):\n",
        "    \"\"\"SMETS distance calculation\"\"\"\n",
        "    if segment1.size == 0 or segment2.size == 0:\n",
        "        return 0, 0, 0\n",
        "\n",
        "    coeffs1 = dwt_transform(segment1)\n",
        "    coeffs2 = dwt_transform(segment2)\n",
        "\n",
        "    if coeffs1.size == 0 or coeffs2.size == 0:\n",
        "        return 0, 0, 0\n",
        "\n",
        "    distances = compute_distances(coeffs1, coeffs2)\n",
        "    d = pair_matching(distances)\n",
        "    p = min(len(segment1), len(segment2))\n",
        "    d_norm = p_norm(d, p)\n",
        "\n",
        "    unmatched = []\n",
        "    m, n = len(segment1), len(segment2)\n",
        "\n",
        "    if m < n:\n",
        "        unmatched = [coeffs2[i] for i in range(m, n)]\n",
        "        EP = entropy_penalty(unmatched, coeffs1)\n",
        "    elif n < m:\n",
        "        unmatched = [coeffs1[i] for i in range(n, m)]\n",
        "        EP = entropy_penalty(unmatched, coeffs2)\n",
        "    else:\n",
        "        EP = 0\n",
        "\n",
        "    P = dimensionality_penalty(m, n)\n",
        "    total_dist = np.sqrt((d_norm + 0.01 * EP) ** 2 + (0.01 * P) ** 2)\n",
        "\n",
        "    logger.info(f\"SMETS: d_norm={d_norm}, EP={EP}, P={P}, total_dist={total_dist}\")\n",
        "    return total_dist, P, EP\n",
        "\n",
        "def compute_smets_to_reference(i, segment, reference_segment):\n",
        "    \"\"\"Compute SMETS distance to reference\"\"\"\n",
        "    total_dist, _, _ = SMETS(segment, reference_segment)\n",
        "    return (i, total_dist)\n",
        "\n",
        "## Interval Processing Functions\n",
        "\n",
        "def get_seizure_intervals_from_parsed(edf_file_info):\n",
        "    \"\"\"Get seizure intervals from parsed summary data\"\"\"\n",
        "    intervals = []\n",
        "    file_name = edf_file_info['file_name']\n",
        "    logger.info(f\"Processing intervals for: {file_name}\")\n",
        "\n",
        "    for start, end in zip(edf_file_info['seizure_start'], edf_file_info['seizure_end']):\n",
        "        intervals.append((float(start), float(end)))\n",
        "        logger.info(f\"Seizure interval: {start} - {end}\")\n",
        "\n",
        "    logger.info(f\"Seizure intervals for {file_name}: {intervals}\")\n",
        "    return intervals\n",
        "\n",
        "def merge_intervals(intervals, max_gap=5):\n",
        "    \"\"\"Merge close intervals\"\"\"\n",
        "    if not intervals:\n",
        "        return []\n",
        "    merged = []\n",
        "    for start, end in sorted(intervals):\n",
        "        if not merged or merged[-1][1] + max_gap < start:\n",
        "            merged.append([start, end])\n",
        "        else:\n",
        "            merged[-1][1] = max(end, merged[-1][1])\n",
        "    return merged\n",
        "\n",
        "def filter_intervals(intervals, min_length=10):\n",
        "    \"\"\"Filter short intervals\"\"\"\n",
        "    return [i for i in intervals if i[1] - i[0] >= min_length]\n",
        "\n",
        "def extend_intervals(intervals, buffer=10, max_time=3600):\n",
        "    \"\"\"Extend intervals with buffer\"\"\"\n",
        "    extended = []\n",
        "    for start, end in intervals:\n",
        "        start = max(0, start - buffer)\n",
        "        end = min(max_time, end + buffer)\n",
        "        extended.append((start, end))\n",
        "    return extended\n",
        "\n",
        "## Evaluation Functions\n",
        "\n",
        "def evaluate_intervals(detected, true_intervals, total_duration):\n",
        "    \"\"\"Evaluate detection performance\"\"\"\n",
        "    logger.info(f\"Evaluating intervals: detected={detected}, true={true_intervals}, duration={total_duration}\")\n",
        "\n",
        "    if not true_intervals:\n",
        "        sensitivity = 1.0\n",
        "        precision = 1.0 if not detected else 0.0\n",
        "        f1 = 2 * (sensitivity * precision) / (sensitivity + precision) if (sensitivity + precision) > 0 else 0.0\n",
        "        logger.info(f\"No true seizures: sensitivity={sensitivity}, precision={precision}, F1={f1}\")\n",
        "        return sensitivity, precision, f1\n",
        "\n",
        "    if not detected:\n",
        "        logger.info(\"No detected seizures: sensitivity=0, precision=0, F1=0\")\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "\n",
        "    for true_start, true_end in true_intervals:\n",
        "        found = False\n",
        "        for det_start, det_end in detected:\n",
        "            overlap_start = max(true_start, det_start)\n",
        "            overlap_end = min(true_end, det_end)\n",
        "            if overlap_start < overlap_end:\n",
        "                tp += overlap_end - overlap_start\n",
        "                found = True\n",
        "        if not found:\n",
        "            fn += true_end - true_start\n",
        "\n",
        "    for det_start, det_end in detected:\n",
        "        overlap_found = False\n",
        "        for true_start, true_end in true_intervals:\n",
        "            overlap_start = max(true_start, det_start)\n",
        "            overlap_end = min(true_end, det_end)\n",
        "            if overlap_start < overlap_end:\n",
        "                overlap_found = True\n",
        "                break\n",
        "        if not overlap_found:\n",
        "            fp += det_end - det_start\n",
        "\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    f1 = 2 * (sensitivity * precision) / (sensitivity + precision) if (sensitivity + precision) > 0 else 0.0\n",
        "\n",
        "    logger.info(f\"TP={tp}, FP={fp}, FN={fn}, sensitivity={sensitivity}, precision={precision}, F1={f1}\")\n",
        "    return sensitivity, precision, f1\n",
        "\n",
        "## Detection Function\n",
        "\n",
        "def detect_seizure_with_smets(segments, window_size, sfreq, max_duration, true_intervals, n_jobs=16):\n",
        "    \"\"\"Detect seizures using SMETS\"\"\"\n",
        "    start_time = time()\n",
        "\n",
        "    # Select reference normal segment\n",
        "    n_reference = min(100, len(segments))\n",
        "    reference_candidates = []\n",
        "\n",
        "    for i in range(n_reference):\n",
        "        segment_start = i * window_size\n",
        "        segment_end = (i + 1) * window_size\n",
        "        is_seizure = any(max(start, segment_start) < min(end, segment_end) for start, end in true_intervals)\n",
        "        if not is_seizure:\n",
        "            reference_candidates.append(segments[i])\n",
        "\n",
        "    if not reference_candidates:\n",
        "        reference_candidates = segments[:n_reference]\n",
        "        logger.warning(\"Could not find non-seizure segments for reference, using first 100 segments\")\n",
        "\n",
        "    amplitudes = [np.max(np.abs(seg)) for seg in reference_candidates]\n",
        "    q25 = np.percentile(amplitudes, 25)\n",
        "    reference_segments = [seg for seg, amp in zip(reference_candidates, amplitudes) if amp < q25]\n",
        "    reference_data = np.mean(reference_segments, axis=0) if reference_segments else segments[0]\n",
        "    logger.info(f\"Selected {len(reference_segments)} reference segments for averaging\")\n",
        "\n",
        "    # Parallel SMETS computation\n",
        "    smets_start = time()\n",
        "    results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
        "        delayed(compute_smets_to_reference)(i, segment, reference_data) for i, segment in enumerate(segments)\n",
        "    )\n",
        "    smets_time = time() - smets_start\n",
        "    logger.info(f\"SMETS computation time: {smets_time:.2f} sec\")\n",
        "\n",
        "    smets_distances = sorted(results, key=lambda x: x[0])\n",
        "    smets_distances = [(i * window_size, dist) for i, dist in smets_distances]\n",
        "\n",
        "    # Smooth distances\n",
        "    distances = [d for _, d in smets_distances]\n",
        "    smoothed_distances = uniform_filter1d(distances, size=3)\n",
        "    smets_distances = [(t, d) for (t, _), d in zip(smets_distances, smoothed_distances)]\n",
        "\n",
        "    # Adaptive threshold based on IQR\n",
        "    q75 = np.percentile(smoothed_distances, 75)\n",
        "    q25 = np.percentile(smoothed_distances, 25)\n",
        "    iqr = q75 - q25\n",
        "    threshold = max(q75 + 2.5 * iqr, 250)\n",
        "    threshold_list = [threshold] * len(smets_distances)\n",
        "    logger.info(f\"Adaptive SMETS threshold: {threshold:.2f} (Q75={q75}, IQR={iqr})\")\n",
        "\n",
        "    # Log max SMETS in seizure region\n",
        "    max_index = len(smoothed_distances) - 1\n",
        "    if true_intervals:\n",
        "        start = min(t[0] for t in true_intervals)\n",
        "        end = max(t[1] for t in true_intervals)\n",
        "        seizure_range = range(max(0, int(start / window_size)), min(max_index + 1, int(end / window_size) + 1))\n",
        "    else:\n",
        "        seizure_range = range(max(0, len(smets_distances) - 50), min(max_index + 1, len(smets_distances)))\n",
        "\n",
        "    max_smets_in_seizure = max((smoothed_distances[i] for i in seizure_range if i <= max_index), default=0)\n",
        "    exceed_points = [(i * window_size, dist) for i, (t, dist) in enumerate(smets_distances) if dist > threshold]\n",
        "    logger.info(f\"Max SMETS near seizure: {max_smets_in_seizure:.2f}\")\n",
        "    logger.info(f\"SMETS threshold exceed points: {exceed_points}\")\n",
        "\n",
        "    # Extract seizure intervals\n",
        "    seizure_intervals = []\n",
        "    start = None\n",
        "    for (idx, _), dist in zip(smets_distances, smoothed_distances):\n",
        "        if dist > threshold and start is None:\n",
        "            start = idx\n",
        "        elif dist <= threshold and start is not None:\n",
        "            if idx - start >= 10:\n",
        "                seizure_intervals.append((start, idx))\n",
        "            start = None\n",
        "\n",
        "    if start is not None and len(segments) * window_size - start >= 10:\n",
        "        seizure_intervals.append((start, len(segments) * window_size))\n",
        "\n",
        "    # Merge, filter and extend intervals\n",
        "    logger.info(f\"Intermediate intervals before extension: {seizure_intervals}\")\n",
        "    seizure_intervals = merge_intervals(seizure_intervals, max_gap=5)\n",
        "    seizure_intervals = filter_intervals(seizure_intervals, min_length=10)\n",
        "    seizure_intervals = extend_intervals(seizure_intervals, buffer=10, max_time=max_duration)\n",
        "    logger.info(f\"Intervals after extension: {seizure_intervals}\")\n",
        "\n",
        "    logger.info(f\"Total processing time in detect_seizure_with_smets: {time() - start_time:.2f} sec\")\n",
        "    return smets_distances, threshold_list, seizure_intervals\n",
        "\n",
        "## Main Processing Function\n",
        "\n",
        "def process_file_from_physionet(edf_file_info, patient_id, base_temp_path, window_size, n_jobs, sampling_rate=256):\n",
        "    \"\"\"Process a single EDF file from PhysioNet\"\"\"\n",
        "    start_time = time()\n",
        "    file_name = edf_file_info['file_name']\n",
        "    logger.info(f\"Processing file: {file_name} for patient {patient_id}\")\n",
        "\n",
        "    # Download EDF file from PhysioNet\n",
        "    os.chdir(base_temp_path)\n",
        "    edf_directory = f'/physionet.org/files/chbmit/1.0.0/{patient_id}/'\n",
        "    edf_file_link = f\"https:/}{edf_directory}{file_name}\"\n",
        "\n",
        "    try:\n",
        "        !wget -r -c -N -np {edf_file_link}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error downloading {file_name}: {e}\")\n",
        "        return 0.0, 0.0, 0.0, time() - start_time, file_name\n",
        "\n",
        "    edf_file_path = f\"{base_temp_path}{edf_directory}{file_name}\"\n",
        "\n",
        "    if not os.path.exists(edf_file_path):\n",
        "        logger.error(f\"Downloaded file not found: {edf_file_path}\")\n",
        "        return 0.0, 0.0, 0.0, time() - start_time, file_name\n",
        "\n",
        "    # Read EDF file\n",
        "    try:\n",
        "        signals, labels = read_edf(edf_file_path)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading {edf_file_path}: {e}\")\n",
        "        return 0.0, 0.0, 0.0, time() - start_time, file_name\n",
        "\n",
        "    max_duration = signals.shape[1] / sampling_rate\n",
        "    logger.info(f\"Recording duration: {max_duration} seconds\")\n",
        "\n",
        "    # Process signals\n",
        "    signals = filter_artifacts(signals, sampling_rate)\n",
        "    signals = normalize_data(signals)\n",
        "    segments = segment_data(signals, sampling_rate, window_size)\n",
        "    segments = quality_control(segments)\n",
        "    logger.info(f\"Number of segments after quality control: {len(segments)}\")\n",
        "\n",
        "    # Get seizure intervals from parsed data\n",
        "    true_intervals = []\n",
        "    if edf_file_info['num_seizures'] > 0:\n",
        "        true_intervals = get_seizure_intervals_from_parsed(edf_file_info)\n",
        "\n",
        "    # Detect seizures\n",
        "    smets_distances, threshold, detected_intervals = detect_seizure_with_smets(\n",
        "        segments, window_size, sampling_rate, max_duration, true_intervals, n_jobs\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Detected seizure intervals: {detected_intervals}\")\n",
        "    logger.info(f\"True seizure intervals: {true_intervals}\")\n",
        "\n",
        "    # Evaluate\n",
        "    sensitivity, precision, f1 = evaluate_intervals(detected_intervals, true_intervals, max_duration)\n",
        "\n",
        "    # Visualization\n",
        "    times = [t for t, _ in smets_distances]\n",
        "    distances = [d for _, d in smets_distances]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(times, distances, label='SMETS distance')\n",
        "    plt.plot(times, threshold, 'r--', label='Adaptive threshold')\n",
        "\n",
        "    for start, end in detected_intervals:\n",
        "        plt.axvspan(start, end, color='yellow', alpha=0.3, label='Detected seizure' if start == detected_intervals[0][0] else \"\")\n",
        "\n",
        "    for start, end in true_intervals:\n",
        "        plt.axvspan(start, end, color='green', alpha=0.2, label='True seizure' if start == true_intervals[0][0] else \"\")\n",
        "\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('SMETS distance')\n",
        "    plt.title(f'Seizure Detection using SMETS ({file_name})')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save to Google Drive\n",
        "    output_path = f'/content/drive/MyDrive/edf_process/EEG_SMETS/smets_plot_{file_name}.png'\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "\n",
        "    # Log results\n",
        "    with open('/content/drive/MyDrive/edf_process/EEG_SMETS/generalized_processing_log.txt', 'a') as f:\n",
        "        f.write(f\"File: {file_name}\\n\")\n",
        "        f.write(f\"Detected intervals: {detected_intervals}\\n\")\n",
        "        f.write(f\"True intervals: {true_intervals}\\n\")\n",
        "        f.write(f\"Sensitivity: {sensitivity:.2f}, Precision: {precision:.2f}, F1: {f1:.2f}\\n\")\n",
        "        f.write(f\"Processing time: {time() - start_time:.2f} seconds\\n\\n\")\n",
        "\n",
        "    # Cleanup temporary files\n",
        "    try:\n",
        "        shutil.rmtree(f'{base_temp_path}/physionet.org')\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return sensitivity, precision, f1, time() - start_time, file_name\n",
        "\n",
        "## Main Execution\n",
        "\n",
        "def process_chb_mit_data_colab(patient_ids=None, window_size=1, n_jobs=16, max_files_per_type=2):\n",
        "    \"\"\"Process CHB-MIT data from PhysioNet in Google Colab\"\"\"\n",
        "    total_start_time = time()\n",
        "\n",
        "    # Setup paths\n",
        "    base_temp_path = '/content'\n",
        "    output_base = '/content/drive/MyDrive/edf_process/EEG_SMETS'\n",
        "    os.makedirs(output_base, exist_ok=True)\n",
        "\n",
        "    if patient_ids is None:\n",
        "        patient_ids = ['chb01']  # Default to chb01\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for patient_id in patient_ids:\n",
        "        logger.info(f\"Processing patient: {patient_id}\")\n",
        "\n",
        "        # Download summary file\n",
        "        os.chdir(base_temp_path)\n",
        "        summary_file_path = f'/physionet.org/files/chbmit/1.0.0/{patient_id}/{patient_id}-summary.txt'\n",
        "        physionet_summary_url = f'https:/{summary_file_path}'\n",
        "\n",
        "        try:\n",
        "            !wget -r -c -N --no-parent -np {physionet_summary_url}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error downloading summary for {patient_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Parse summary\n",
        "        full_summary_path = f'{base_temp_path}{summary_file_path}'\n",
        "        if not os.path.exists(full_summary_path):\n",
        "            logger.error(f\"Summary file not found: {full_summary_path}\")\n",
        "            continue\n",
        "\n",
        "        edf_files_list = parse_summary_file(full_summary_path)\n",
        "\n",
        "        # Separate seizure and normal files\n",
        "        seizure_files = [f for f in edf_files_list if f['num_seizures'] > 0]\n",
        "        normal_files = [f for f in edf_files_list if f['num_seizures'] == 0]\n",
        "\n",
        "        logger.info(f\"Patient {patient_id}: {len(seizure_files)} seizure files, {len(normal_files)} normal files\")\n",
        "\n",
        "        # Limit files if specified\n",
        "        if max_files_per_type:\n",
        "            seizure_files = seizure_files[:max_files_per_type]\n",
        "            normal_files = normal_files[:max_files_per_type]\n",
        "\n",
        "        files_to_process = seizure_files + normal_files\n",
        "\n",
        "        # Process files\n",
        "        for edf_file in files_to_process:\n",
        "            sens, prec, f1, proc_time, file_name = process_file_from_physionet(\n",
        "                edf_file, patient_id, base_temp_path, window_size, n_jobs\n",
        "            )\n",
        "            all_results.append((file_name, sens, prec, f1, proc_time))\n",
        "\n",
        "    # Save summary\n",
        "    summary_path = f'{output_base}/generalized_processing_summary.txt'\n",
        "    with open(summary_path, 'w') as f:\n",
        "        f.write(\"Final Results Table\\n\")\n",
        "        f.write(\"==================\\n\")\n",
        "        f.write(\"File\\tSensitivity\\tPrecision\\tF1\\tProcessing Time (s)\\n\")\n",
        "        for file_name, sens, prec, f1, t in all_results:\n",
        "            f.write(f\"{file_name}\\t{sens:.2f}\\t\\t{prec:.2f}\\t\\t{f1:.2f}\\t\\t{t:.2f}\\n\")\n",
        "        f.write(f\"\\nTotal processing time: {time() - total_start_time:.2f} seconds\\n\")\n",
        "\n",
        "    logger.info(f\"Total processing time for all files: {time() - total_start_time:.2f} seconds\")\n",
        "    return all_results\n",
        "\n",
        "# Run processing\n",
        "if __name__ == \"__main__\":\n",
        "    # Process patients chb01, chb02, chb03 (example)\n",
        "    patient_ids = ['chb01', 'chb02', 'chb03']\n",
        "    process_chb_mit_data_colab(patient_ids=patient_ids, window_size=1, n_jobs=16, max_files_per_type=2)"
      ],
      "metadata": {
        "id": "LZpd8WQRO4DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# code 3"
      ],
      "metadata": {
        "id": "32ldVTOnO5z3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MwBCIfgePA-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}